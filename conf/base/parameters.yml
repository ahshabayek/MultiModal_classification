# Data Processing Parameters
data_processing:
  # Directory where images will be stored
  data_dir: "data/01_raw/hateful_memes"
  # Google Drive URL for image archive
  gdrive_img_url: "https://drive.google.com/uc?id=1VZ2WQrh4MRStFfWRSx0ezYJ_DlcaCGwI"
  # Use HuggingFace validation split as validation set
  use_dev_as_val: true
  # If not using dev as val, split this ratio from train
  val_split_ratio: 0.1
  # Random seed for reproducibility
  random_seed: 42
  # Text preprocessing
  max_text_length: 512
  lowercase: false

  # Caption Enrichment (CES) - improves AUROC by +2-6%
  # Reference: "Caption Enriched Samples for Improving Hateful Memes Detection" (EMNLP 2021)
  use_captions: false # Set to true to enable (requires BLIP model download)
  caption_cache_path: "data/02_intermediate/captions.csv"

# Training Parameters (Facebook MMF baseline + improvements)
# Reference: https://github.com/facebookresearch/mmf/issues/290
# =============================================================================
# TUNING EXPERIMENT LOG
# =============================================================================
# This section documents hyperparameter tuning experiments.
#
# Baseline Results (before tuning):
#   vilbert_train (HF, 5e-5, focal): 0.6645 AUROC
#   vilbert_frcnn_train (5e-5, focal): 0.6235 AUROC
#   vilbert_vg_train (1e-5, CE): 0.6367 AUROC
#
# Tuning Strategy:
#   1. Standardize to 1e-5 + CE (matches best-performing models)
#   2. Test different warmup periods
#   3. Compare results
# =============================================================================

training:
  # Batch size: 32 is MMF default, adjust based on GPU memory
  batch_size: 32
  # Number of epochs (with early stopping)
  num_epochs: 20
  # TUNED: Changed from 5e-5 to 1e-5 (matches Facebook models)
  learning_rate: 1.0e-5
  # Weight decay for AdamW
  weight_decay: 0.01
  # Warmup steps: 2000 is MMF default
  warmup_steps: 2000
  # Early stopping patience (epochs without improvement)
  early_stopping_patience: 5
  # Gradient clipping max norm
  gradient_clip: 1.0

  # TUNED: Changed from focal to CE (matches Facebook models)
  loss_type: "ce"
  # Focal loss alpha: weight for minority class (~35% hateful in training)
  focal_alpha: 0.35
  # Focal loss gamma: focusing parameter (higher = more focus on hard examples)
  focal_gamma: 2.0
  # Label smoothing factor (0.1 is typical)
  label_smoothing: 0.0
  # Linear decay schedule
  use_linear_decay: true

# ViLBERT Model Parameters (HuggingFace version - current baseline)
vilbert:
  # HuggingFace model for pretrained weights
  huggingface_model: "visualjoyce/transformers4vl-vilbert"
  num_labels: 2
  # Number of BERT layers to freeze (0 = train all, 6 = freeze first 6)
  # For best results, set to 0 (train all layers)
  freeze_bert_layers: 0
  max_seq_length: 128

  # Visual feature extraction
  # Options: "resnet" (faster, ~0.65 AUROC) or "clip" (slower, ~0.68-0.70 AUROC)
  feature_extractor: "resnet"
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 224

  # Directory to save trained models
  output_dir: "data/05_model_output"
  # Path to locally trained checkpoint
  checkpoint_path: "data/05_model_output/vilbert_best.pt"

# ViLBERT with Faster R-CNN Features + Facebook Official Weights
# Use this for comparison with Facebook's baseline (expected AUROC ~0.70)
# Run with: kedro run --pipeline=vilbert_frcnn_train
vilbert_frcnn:
  # Path to Facebook's official pretrained weights (Conceptual Captions)
  # Download with: python scripts/download_weights.py --source vilbert_cc --output ./weights/
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128

  # Faster R-CNN feature extraction (object-based, similar to Facebook's setup)
  # Note: Facebook used ResNeXt-152 on Visual Genome, we use ResNet-50-FPN on COCO
  feature_extractor: "fasterrcnn"
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 224
  # Confidence threshold for object detection (lower = more regions from detected objects)
  frcnn_confidence_threshold: 0.2

  # Output directory for FRCNN model
  output_dir: "data/05_model_output/frcnn"
  checkpoint_path: "data/05_model_output/frcnn/vilbert_frcnn_best.pt"

# Training parameters for FRCNN model (can be customized separately)
# TUNING EXPERIMENT 2: Try 5e-5 + CE (higher LR, different loss)
# Original: 0.6235 AUROC with 5e-5 + focal
# Experiment 1 (1e-5 + CE): Not run yet
# Experiment 2 (5e-5 + CE): Testing now
training_frcnn:
  batch_size: 32
  num_epochs: 20
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with Faster R-CNN ResNet-152 FPN Features + Facebook Official Weights
# This uses a stronger ResNet-152 backbone (vs ResNet-50 in vilbert_frcnn)
# to test if a deeper backbone improves feature quality for ViLBERT.
# Expected AUROC: ~0.65-0.68 (potentially better than ResNet-50 FRCNN's 0.6235)
# Run with: kedro run --pipeline=vilbert_frcnn_resnet152_train
vilbert_frcnn_resnet152:
  # Path to Facebook's official pretrained weights (Conceptual Captions)
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128

  # Faster R-CNN ResNet-152 FPN feature extraction
  # Uses ResNet-152 backbone (vs ResNet-50 in standard FRCNN)
  # Detection head initialized from ResNet-50 FPN v2 COCO weights
  feature_extractor: "fasterrcnn_resnet152"
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 224
  # Confidence threshold for object detection
  frcnn_confidence_threshold: 0.2

  # Output directory for ResNet-152 FRCNN model
  output_dir: "data/05_model_output/frcnn_resnet152"
  checkpoint_path: "data/05_model_output/frcnn_resnet152/vilbert_frcnn_resnet152_best.pt"

# Training parameters for ResNet-152 FRCNN model
# Using same settings as LMDB (which achieved best results) for fair comparison
training_frcnn_resnet152:
  batch_size: 32
  num_epochs: 20
  # Using lower LR like LMDB (better results than 5e-5)
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  # Using CE loss like LMDB (better than focal for this task)
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with DINOv2 Vision Transformer Features
# This configuration uses Meta's DINOv2 self-supervised ViT for visual features.
# DINOv2 provides semantically rich patch features without object detection domain bias.
# Expected AUROC: ~0.68-0.72 (better than COCO FRCNN, potentially matching grid features)
# Run with: kedro run --pipeline=vilbert_dinov2_train
vilbert_dinov2:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  # FREEZE EXPERIMENT RESULT (fixed): freeze=6 had negligible effect (-0.03%)
  # Keeping freeze=0 as default
  freeze_bert_layers: 0
  max_seq_length: 128

  # DINOv2 Vision Transformer feature extraction
  feature_extractor: "dinov2"
  # Model size: "small" (384-dim), "base" (768-dim), "large" (1024-dim), "giant" (1536-dim)
  dinov2_model_size: "large"
  # 36 regions works best for DINOv2 (100 regions causes overfitting)
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 518 # DINOv2 uses 518x518 input

  # Region selection strategy:
  # - "interpolate": Bilinear interpolation from patch grid (baseline: 0.7056 AUROC)
  # - "attention": Select top-K patches by self-attention score (recommended)
  region_selection: "attention"

  # Output directory
  output_dir: "data/05_model_output/dinov2"

# Training config for DINOv2
# TUNING EXPERIMENT: Try higher LR since only projection trains
# Original (1e-5): 0.7056 AUROC - good but projection layer may need more signal
# Experiment: 5e-5 + focal loss (DINOv2 frozen, only projection trains)
training_dinov2:
  # BATCH SIZE TUNING RESULTS:
  # Batch=16: 0.7069 AUROC (+0.13%) <- BEST
  # Batch=32: 0.7056 AUROC (original)
  # Batch=64: 0.6847 AUROC (-2.09%)
  batch_size: 16
  num_epochs: 20
  # Original best settings (1e-5, CE)
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  # LABEL SMOOTHING TUNING RESULTS:
  # 0.0: 0.7069 AUROC <- BEST
  # 0.1: 0.6952 AUROC (-1.17%)
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with DINOv2 Multi-Layer Feature Fusion
# This configuration extracts features from multiple DINOv2 transformer layers
# and fuses them to capture both low-level patterns and high-level semantics.
# Hypothesis: Earlier layers capture text-in-image patterns, later layers capture semantics.
# Expected AUROC: ~0.71-0.73 (better than single-layer 0.7056)
# Run with: kedro run --pipeline=vilbert_dinov2_multilayer_train
vilbert_dinov2_multilayer:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  # FREEZE EXPERIMENT RESULT (fixed): freeze=6 had negligible effect (-0.03%)
  # Keeping freeze=0 as default
  freeze_bert_layers: 0
  max_seq_length: 128

  # DINOv2 Multi-Layer feature extraction
  feature_extractor: "dinov2_multilayer"
  # Model size: "small", "base", "large", "giant"
  dinov2_model_size: "large"
  # Layers to extract features from (1-indexed)
  # For ViT-L (24 layers): [6, 12, 18, 24] captures low to high level
  dinov2_layer_indices: [6, 12, 18, 24]
  # Fusion strategy: "concat", "weighted_sum", "attention"
  # - concat: Concatenate all layers then project (simple, effective)
  # - weighted_sum: Learn weights for each layer (fewer params)
  # - attention: Cross-layer attention pooling (most expressive)
  dinov2_fusion_strategy: "concat"
  # Number of regions (36 = 6x6 grid)
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 518

  # Output directory
  output_dir: "data/05_model_output/dinov2_multilayer"

# Training config for DINOv2 Multi-Layer
training_dinov2_multilayer:
  batch_size: 32
  num_epochs: 20
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  # LABEL SMOOTHING TUNING RESULTS:
  # 0.0: 0.7067 AUROC
  # 0.1: 0.7171 AUROC (+1.04%) <- BEST
  label_smoothing: 0.1
  use_linear_decay: true

# ViLBERT with Visual Genome Faster R-CNN Features
# This configuration uses Faster R-CNN pretrained on Visual Genome (1600 classes)
# which matches Facebook's original ViLBERT setup more closely than COCO.
# Expected AUROC: ~0.68-0.72 (closer to Facebook's 0.7045 baseline)
# Run with: kedro run --pipeline=vilbert_vg_train
vilbert_vg:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128

  # Visual Genome Faster R-CNN feature extraction
  # Download weights from: https://drive.google.com/file/d/18n_3V1rywgeADZ3oONO0DsuuS9eMW6sN/view
  feature_extractor: "fasterrcnn_vg"
  # Path to Visual Genome pretrained Faster R-CNN weights
  vg_weights_path: "weights/faster_rcnn_res101_vg.pth"
  # Facebook uses 100 regions per image (from detectron.lmdb)
  max_regions: 100
  visual_feature_dim: 2048
  image_size: 224
  # Lower threshold to get more region proposals
  frcnn_confidence_threshold: 0.2
  nms_threshold: 0.3

  # Output directory
  output_dir: "data/05_model_output/vg"
  checkpoint_path: "data/05_model_output/vg/vilbert_vg_best.pt"

# Training parameters for Visual Genome model
# TUNING EXPERIMENT: Try 5e-6 LR with 4000 warmup steps
# Original (1e-5, warmup=2000): 0.6367 AUROC - showed early degradation
# Hypothesis: VG features may need gentler training due to domain mismatch
training_vg:
  batch_size: 32
  # Reduced epochs with early stopping
  num_epochs: 20
  # TUNED: Lower LR to prevent early degradation
  learning_rate: 5.0e-6
  weight_decay: 0.01
  # TUNED: Longer warmup for smoother training
  warmup_steps: 4000
  # Re-enable early stopping
  early_stopping_patience: 5
  gradient_clip: 1.0
  # Keep CE loss
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  # Enable linear decay scheduler (warmup_linear)
  use_linear_decay: true

# ViLBERT with Visual Genome Faster R-CNN + Trained RPN
# This configuration uses the TRAINED RPN from the Visual Genome checkpoint
# instead of grid-based proposals, which should significantly improve results.
# Key difference from vilbert_vg: Uses learned region proposals from VG RPN
# Expected AUROC: ~0.70-0.73 (should be close to LMDB 0.7433)
# Run with: kedro run --pipeline=vilbert_vg_rpn_train
vilbert_vg_rpn:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128

  # Visual Genome Faster R-CNN with trained RPN
  feature_extractor: "fasterrcnn_vg_rpn"
  # Path to Visual Genome pretrained weights (includes RPN)
  vg_weights_path: "weights/faster_rcnn_res101_vg.pth"
  # Use 36 regions (same as DINOv2 best config)
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 600
  # RPN proposal filtering
  nms_threshold: 0.7
  pre_nms_top_n: 6000
  post_nms_top_n: 300

  # Output directory
  output_dir: "data/05_model_output/vg_rpn"
  checkpoint_path: "data/05_model_output/vg_rpn/vilbert_vg_rpn_best.pt"

# Training parameters for VG RPN model
training_vg_rpn:
  batch_size: 32
  num_epochs: 20
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with Simple VG ResNet-101 Backbone (no detection)
# This uses ONLY the ResNet-101 backbone from the VG checkpoint
# with simple grid-based pooling (like ResNet-152 extractor).
# NO RPN, NO ROI pooling, NO detection head.
# Expected AUROC: ~0.67+ (should beat ImageNet ResNet-152's 0.6645)
# Run with: kedro run --pipeline=vilbert_resnet_vg_train
vilbert_resnet_vg:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128

  # Simple VG ResNet-101 with grid features (no detection)
  feature_extractor: "resnet_vg"
  # Path to Visual Genome pretrained weights (only backbone loaded)
  vg_weights_path: "weights/faster_rcnn_res101_vg.pth"
  # 36 regions = 6x6 grid (same as ResNet-152 baseline)
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 224

  # Output directory
  output_dir: "data/05_model_output/resnet_vg"
  checkpoint_path: "data/05_model_output/resnet_vg/vilbert_resnet_vg_best.pt"

# ViLBERT with ResNet-152 + ROI Pooling (ImageNet backbone + detection pipeline)
# This tests whether ROI pooling helps with an ImageNet backbone (no VG/COCO detection training).
# Key hypothesis: ROI pooling on multi-scale proposals may capture better object-centric
# features than simple grid pooling, even without detection pretraining.
# Comparison:
#   - vilbert_train (ResNet-152 + grid): 0.6645 AUROC
#   - vilbert_frcnn_resnet152_train (ResNet-152 + COCO detection): 0.6334 AUROC
#   - This model (ResNet-152 + ROI pooling, no detection): ???
# Run with: kedro run --pipeline=vilbert_resnet152_roi_train
vilbert_resnet152_roi:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  # FREEZE EXPERIMENT RESULT (fixed): freeze=6 had negligible effect (-0.03%)
  # Keeping freeze=0 as default
  freeze_bert_layers: 0
  max_seq_length: 128

  # ResNet-152 with ROI pooling (ImageNet backbone, no detection training)
  feature_extractor: "resnet152_roi"
  # REGION COUNT TUNING RESULTS:
  # 36 regions (6x6): 0.7197 AUROC <- BEST
  # 49 regions (7x7): 0.7008 AUROC (-1.89%)
  # 64 regions (8x8): 0.7141 AUROC (-0.56%)
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 600
  # ROI pooling size (before layer4)
  roi_size: 14
  # Use multi-scale proposals (vs simple grid)
  use_multi_scale: true

  # Output directory
  output_dir: "data/05_model_output/resnet152_roi"
  checkpoint_path: "data/05_model_output/resnet152_roi/vilbert_resnet152_roi_best.pt"

# Training parameters for ResNet-152 ROI model
training_resnet152_roi:
  # BATCH SIZE TUNING RESULTS:
  # Batch=16: 0.7180 AUROC (-0.17%)
  # Batch=32: 0.7197 AUROC <- BEST (original)
  # Batch=64: 0.7141 AUROC (-0.56%)
  batch_size: 32
  num_epochs: 20
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  # LABEL SMOOTHING TUNING RESULTS:
  # 0.0: 0.7197 AUROC <- BEST
  # 0.1: 0.7045 AUROC (-1.52%)
  label_smoothing: 0.0
  use_linear_decay: true

# =============================================================================
# ResNet-152 Grid (Facebook CC weights, NO ROI pooling)
# =============================================================================
# This is a control experiment to isolate the effect of ROI pooling.
# Same ViLBERT weights as vilbert_resnet152_roi, but with simple grid pooling.
# Comparison:
#   - vilbert_resnet152_roi_train (Facebook CC + ROI pooling): 0.7197 AUROC
#   - This model (Facebook CC + grid pooling): ???
# Run with: kedro run --pipeline=vilbert_resnet152_grid_train
vilbert_resnet152_grid:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128

  # ResNet-152 with simple grid pooling (NO ROI)
  feature_extractor: "resnet"
  max_regions: 36
  visual_feature_dim: 2048
  image_size: 224

  # Output directory
  output_dir: "data/05_model_output/resnet152_grid"
  checkpoint_path: "data/05_model_output/resnet152_grid/vilbert_resnet152_grid_best.pt"

# Training parameters for ResNet-152 Grid model (Facebook CC weights)
training_resnet152_grid:
  batch_size: 32
  num_epochs: 20
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# Training parameters for VG ResNet-101 model
training_resnet_vg:
  batch_size: 32
  num_epochs: 20
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with PRECOMPUTED Visual Genome Features (Facebook-style)
# This is the closest match to Facebook's original setup:
# - Features are extracted ONCE and cached in HDF5 (like detectron.lmdb)
# - 100 regions per image with consistent features across epochs
# - No on-the-fly extraction noise
# Run with: kedro run --pipeline=vilbert_precomputed_train
vilbert_precomputed:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128
  # Precomputed features settings
  max_regions: 100
  visual_feature_dim: 2048
  # Paths to precomputed features (generated by scripts/extract_features.py)
  precomputed_features_path: "data/03_features/vg_features_100.h5"
  precomputed_id_map_path: "data/03_features/vg_features_100_id_map.npy"
  # Output directory
  output_dir: "data/05_model_output/precomputed"
  checkpoint_path: "data/05_model_output/precomputed/vilbert_precomputed_best.pt"

# Training parameters for precomputed features model
# Facebook MMF exact settings
training_precomputed:
  batch_size: 32
  # Facebook uses max_updates=22000
  num_epochs: 83
  # Facebook exact LR: 1e-5
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  # Early stopping when no improvement
  early_stopping_patience: 10
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with Facebook's Official LMDB Features (detectron.lmdb)
# This uses Facebook's EXACT precomputed features extracted with ResNeXt-152
# on Visual Genome (the same features used in their baseline).
# Expected AUROC: ~0.70 (matching Facebook's 0.7045 baseline)
# Run with: kedro run --pipeline=vilbert_lmdb_train
vilbert_lmdb:
  # FOCAL LOSS EXPERIMENT
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  # FREEZE EXPERIMENT RESULT (fixed): freeze=6 had negligible effect (-0.03%)
  # Keeping freeze=0 as default
  freeze_bert_layers: 0
  max_seq_length: 128
  # Facebook's official LMDB features (extracted with ResNeXt-152 on VG)
  # Features: 2048-dim from ResNeXt-152-32x8d with attribute loss
  # 100 regions per image, same as Facebook baseline
  lmdb_path: "data/03_features/mmf/detectron.lmdb"
  max_regions: 100
  visual_feature_dim: 2048
  # Output directory
  output_dir: "data/05_model_output/lmdb"
  checkpoint_path: "data/05_model_output/lmdb/vilbert_lmdb_best.pt"

# Training parameters for LMDB model
# Facebook MMF exact settings
training_lmdb:
  # BATCH SIZE TUNING RESULTS:
  # Batch=16: 0.7580 AUROC (+1.47%) <- BEST
  # Batch=32: 0.7433 AUROC (original)
  # Batch=64: 0.7434 AUROC
  batch_size: 16
  # Reduced epochs - best AUROC achieved by epoch 7
  num_epochs: 20
  # Facebook exact LR: 1e-5
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  # Early stopping when no improvement
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true

# ViLBERT with Facebook's X-152++ Features (grid-feats-vqa)
# This uses Facebook's state-of-the-art X-152++ model from grid-feats-vqa
# which won the 2020 VQA Challenge. Features are extracted on-the-fly.
# Expected AUROC: ~0.72-0.75 (potentially better than LMDB)
# Run with: kedro run --pipeline=vilbert_x152_train
# Requires: pip install 'git+https://github.com/facebookresearch/detectron2.git'
vilbert_x152:
  # Path to Facebook's official pretrained weights
  facebook_weights_path: "weights/vilbert_pretrained_cc.bin"
  num_labels: 2
  freeze_bert_layers: 0
  max_seq_length: 128
  # X-152++ feature extraction (state-of-the-art from 2020 VQA Challenge)
  # Download weights: https://dl.fbaipublicfiles.com/grid-feats-vqa/X-152pp/X-152pp.pth
  feature_extractor: "grid_x152"
  x152_weights_path: "weights/X-152pp.pth"
  # Use 100 regions like Facebook baseline
  max_regions: 100
  visual_feature_dim: 2048
  # Detection thresholds
  confidence_threshold: 0.2
  nms_threshold: 0.5
  # Auto-download weights if not found
  auto_download_weights: true
  # Output directory
  output_dir: "data/05_model_output/x152"
  checkpoint_path: "data/05_model_output/x152/vilbert_x152_best.pt"

# Training parameters for X-152++ model
# Same as LMDB settings since we want to compare feature quality
training_x152:
  batch_size: 32
  num_epochs: 20
  # Facebook exact LR: 1e-5
  learning_rate: 1.0e-5
  weight_decay: 0.01
  warmup_steps: 2000
  early_stopping_patience: 5
  gradient_clip: 1.0
  loss_type: "ce"
  focal_alpha: 0.35
  focal_gamma: 2.0
  label_smoothing: 0.0
  use_linear_decay: true
