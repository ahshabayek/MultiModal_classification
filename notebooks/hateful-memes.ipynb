{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress the specific Hugging Face token warning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Error while fetching `HF_TOKEN` secret value from your vault\"\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the public dataset\n",
    "dataset = load_dataset(\"neuralcatcher/hateful_memes\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Remove duplicates\n",
    "for i_split, i_data in dataset.items():\n",
    "    dataset[i_split] = Dataset.from_pandas(\n",
    "        pd.DataFrame(i_data).drop_duplicates(), preserve_index=False\n",
    "    )\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "total_set = set()\n",
    "for i_split, i_data in dataset.items():\n",
    "    print(i_split)\n",
    "    split_set = set()\n",
    "    for i_sample in i_data:\n",
    "        if i_sample[\"id\"] in split_set:\n",
    "            print(f\"duplicate id: {i_sample['id']} in split set\")\n",
    "        else:\n",
    "            split_set.add(i_sample[\"id\"])\n",
    "            if i_sample[\"id\"] in total_set:\n",
    "                print(f\"duplicate id: {i_sample['id']} in total set\")\n",
    "            else:\n",
    "                total_set.add(i_sample[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for split, data in dataset.items():\n",
    "    print(f\"{split}: {len(data)} examples\")\n",
    "    print(dataset[split].features)\n",
    "    total_bytes = (\n",
    "        sum(sys.getsizeof(dataset[split][i]) for i in range(100))\n",
    "        * len(dataset[split])\n",
    "        // 100\n",
    "    )\n",
    "    print(f\"Approximate size of {split} split: {total_bytes / 1e6:.2f} MB\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract img archive\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import tarfile\n",
    "\n",
    "if not os.path.exists(\"img/\"):\n",
    "    gdown.download(\n",
    "        \"https://drive.google.com/uc?id=1VZ2WQrh4MRStFfWRSx0ezYJ_DlcaCGwI\",\n",
    "        \"img.tar.gz\",\n",
    "        quiet=False,\n",
    "    )\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(\"img.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall()  # extracts into ./img/ if archive contains a img/ folder\n",
    "    print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch missing images if any\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "base_url = (\n",
    "    \"https://huggingface.co/datasets/limjiayi/hateful_memes_expanded/resolve/main\"\n",
    ")\n",
    "\n",
    "for i_split, i_data in dataset.items():\n",
    "    for i_sample in i_data:\n",
    "        if not os.path.exists(i_sample[\"img\"]):\n",
    "            response = requests.get(f\"{base_url}/{i_sample['img']}\")\n",
    "            response.raise_for_status()\n",
    "            with open(i_sample[\"img\"], \"wb\") as f:\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef473e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn img dirs into PIL objects and load them to confirm their existence\n",
    "\n",
    "from datasets import Image as HFImage\n",
    "\n",
    "dataset_dir = os.path.abspath(\".\")\n",
    "\n",
    "\n",
    "# Turn relative dirs into absolute\n",
    "def fix_paths(example):\n",
    "    example[\"img\"] = os.path.join(dataset_dir, example[\"img\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(fix_paths)\n",
    "dataset = dataset.cast_column(\"img\", HFImage())\n",
    "\n",
    "for i_split, i_data in dataset.items():\n",
    "    for i_sample in i_data:\n",
    "        i_sample[\"img\"].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea26386",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b36205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper func: extract text from meme (optional)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "def extract_text(image):\n",
    "    image = np.array(image)\n",
    "    if image.ndim == 3 and image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "    image = cv2.bilateralFilter(image, 5, 55, 60)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, image = cv2.threshold(image, 240, 255, 1)\n",
    "\n",
    "    custom_config = (\n",
    "        r\"--oem 3 --psm 11 -c tessedit_char_whitelist= 'ABCDEFGHIJKLMNOPQRSTUVWXYZ '\"\n",
    "    )\n",
    "    text = pytesseract.image_to_string(image, lang=\"eng\", config=custom_config)\n",
    "    return text.replace(\"\\n\", \" \").replace(\"  \", \" \").rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate text in memes (optional)\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "index = 70\n",
    "text_ref = dataset[\"train\"][index][\"text\"]\n",
    "text_ext = extract_text(dataset[\"train\"][index][\"img\"])\n",
    "ratio = SequenceMatcher(None, text_ref, text_ext).ratio()\n",
    "while ratio > 0.60:\n",
    "    index += 1\n",
    "    text_ref = dataset[\"train\"][index][\"text\"]\n",
    "    text_ext = extract_text(dataset[\"train\"][index][\"img\"])\n",
    "    ratio = SequenceMatcher(None, text_ref, text_ext).ratio()\n",
    "\n",
    "# Print the extracted text\n",
    "print(index)\n",
    "print(ratio)\n",
    "print(text_ref)\n",
    "print(text_ext)\n",
    "dataset[\"train\"][index][\"img\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
